{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltd53TlCJVum"
      },
      "outputs": [],
      "source": [
        "# authors\n",
        "# Vivek Raj, Srimadhaven Thirumurthy\n",
        "# Team 1\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from pandas.core.window.expanding import Axis\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1B-KYjiqNWAaIAOFabm82DWK0VAASNSxJ\n",
        "!wget -O testing_set1.pkl \"https://drive.google.com/uc?export=download&id=1d4qJcDmrJ8HLT4hvyMFNDZp2v8l0uh08\"\n",
        "!wget -O testing_set2.pkl \"https://drive.google.com/uc?export=download&id=14SrA_AtrM9JugcMCgYLl33elD-j7JZgF\"\n",
        "!wget -O testing_set3.pkl \"https://drive.google.com/uc?export=download&id=11dXK2OgnrWw_K-QfxGtEtMiY8Ubay6YT\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty75nj9scFma",
        "outputId": "f507e7c0-7a8c-4dc1-f429-61cc4caa3bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1B-KYjiqNWAaIAOFabm82DWK0VAASNSxJ\n",
            "To: /content/training_set.pkl\n",
            "100% 177M/177M [00:04<00:00, 35.9MB/s]\n",
            "--2023-05-05 01:04:41--  https://drive.google.com/uc?export=download&id=1d4qJcDmrJ8HLT4hvyMFNDZp2v8l0uh08\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.203.138, 74.125.203.139, 74.125.203.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.203.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-5o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8crhifdpsaajeuahosbht9ohvpgsrdka/1683248625000/11326793013295524174/*/1d4qJcDmrJ8HLT4hvyMFNDZp2v8l0uh08?e=download&uuid=423ca204-6e93-4ae3-a019-33d3f069184c [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-05 01:04:45--  https://doc-08-5o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8crhifdpsaajeuahosbht9ohvpgsrdka/1683248625000/11326793013295524174/*/1d4qJcDmrJ8HLT4hvyMFNDZp2v8l0uh08?e=download&uuid=423ca204-6e93-4ae3-a019-33d3f069184c\n",
            "Resolving doc-08-5o-docs.googleusercontent.com (doc-08-5o-docs.googleusercontent.com)... 74.125.204.132, 2404:6800:4008:c04::84\n",
            "Connecting to doc-08-5o-docs.googleusercontent.com (doc-08-5o-docs.googleusercontent.com)|74.125.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87712028 (84M) [application/octet-stream]\n",
            "Saving to: ‘testing_set1.pkl’\n",
            "\n",
            "testing_set1.pkl    100%[===================>]  83.65M  73.6MB/s    in 1.1s    \n",
            "\n",
            "2023-05-05 01:04:47 (73.6 MB/s) - ‘testing_set1.pkl’ saved [87712028/87712028]\n",
            "\n",
            "--2023-05-05 01:04:47--  https://drive.google.com/uc?export=download&id=14SrA_AtrM9JugcMCgYLl33elD-j7JZgF\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.203.138, 74.125.203.139, 74.125.203.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.203.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0k-5o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t7bflcpt5l8smuej9p3q9bhijq7k8r7c/1683248625000/11326793013295524174/*/14SrA_AtrM9JugcMCgYLl33elD-j7JZgF?e=download&uuid=8ac316f4-0ccb-471f-abc6-d90164cd7415 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-05 01:04:50--  https://doc-0k-5o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t7bflcpt5l8smuej9p3q9bhijq7k8r7c/1683248625000/11326793013295524174/*/14SrA_AtrM9JugcMCgYLl33elD-j7JZgF?e=download&uuid=8ac316f4-0ccb-471f-abc6-d90164cd7415\n",
            "Resolving doc-0k-5o-docs.googleusercontent.com (doc-0k-5o-docs.googleusercontent.com)... 74.125.204.132, 2404:6800:4008:c04::84\n",
            "Connecting to doc-0k-5o-docs.googleusercontent.com (doc-0k-5o-docs.googleusercontent.com)|74.125.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62300682 (59M) [application/octet-stream]\n",
            "Saving to: ‘testing_set2.pkl’\n",
            "\n",
            "testing_set2.pkl    100%[===================>]  59.41M  63.4MB/s    in 0.9s    \n",
            "\n",
            "2023-05-05 01:04:52 (63.4 MB/s) - ‘testing_set2.pkl’ saved [62300682/62300682]\n",
            "\n",
            "--2023-05-05 01:04:52--  https://drive.google.com/uc?export=download&id=11dXK2OgnrWw_K-QfxGtEtMiY8Ubay6YT\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.203.138, 74.125.203.139, 74.125.203.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.203.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-00-5o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/irfp21rik6ajca48nu314ieca5frk7v6/1683248625000/11326793013295524174/*/11dXK2OgnrWw_K-QfxGtEtMiY8Ubay6YT?e=download&uuid=b87e37bb-fb7a-44a4-b3be-816be2751560 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-05 01:04:57--  https://doc-00-5o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/irfp21rik6ajca48nu314ieca5frk7v6/1683248625000/11326793013295524174/*/11dXK2OgnrWw_K-QfxGtEtMiY8Ubay6YT?e=download&uuid=b87e37bb-fb7a-44a4-b3be-816be2751560\n",
            "Resolving doc-00-5o-docs.googleusercontent.com (doc-00-5o-docs.googleusercontent.com)... 74.125.204.132, 2404:6800:4008:c04::84\n",
            "Connecting to doc-00-5o-docs.googleusercontent.com (doc-00-5o-docs.googleusercontent.com)|74.125.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 91003605 (87M) [application/octet-stream]\n",
            "Saving to: ‘testing_set3.pkl’\n",
            "\n",
            "testing_set3.pkl    100%[===================>]  86.79M  44.0MB/s    in 2.0s    \n",
            "\n",
            "2023-05-05 01:04:59 (44.0 MB/s) - ‘testing_set3.pkl’ saved [91003605/91003605]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO57WDfUJOQv"
      },
      "outputs": [],
      "source": [
        "training_object = open(\"/content/training_set.pkl\", \"rb\")\n",
        "training_df = pickle.load(training_object)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RpB7do11hJf"
      },
      "outputs": [],
      "source": [
        "stock_df = pd.DataFrame()\n",
        "# number of table to concat\n",
        "limit = 15\n",
        "# Loop through the list of tables and concatenate them vertically into stock_df\n",
        "for i in range (limit):\n",
        "    table = training_df[i]\n",
        "    stock_df = pd.concat([stock_df, table], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cGbXen7AdRQT",
        "outputId": "b3683137-3b23-4baa-d852-2dacf0e62c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Open      High       Low     Close    Volume\n",
              "0      0.672106  0.679902  0.669265  0.671966  0.336899\n",
              "1      0.672106  0.674597  0.670546  0.672106  0.208023\n",
              "2      0.672106  0.678621  0.671827  0.676282  0.142405\n",
              "3      0.676338  0.677340  0.672384  0.673498  0.146400\n",
              "4      0.673442  0.676004  0.672997  0.675892  0.116960\n",
              "...         ...       ...       ...       ...       ...\n",
              "33025  0.921706  0.923321  0.916412  0.916921  0.291367\n",
              "33026  0.916921  0.918476  0.910940  0.913751  0.294602\n",
              "33027  0.913571  0.916024  0.909923  0.911119  0.261749\n",
              "33028  0.911149  0.911717  0.902748  0.904480  0.302225\n",
              "33029  0.904779  0.909085  0.901430  0.904719  0.684395\n",
              "\n",
              "[33030 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e5f2d786-241a-4fb8-91d0-d482c88f1088\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.672106</td>\n",
              "      <td>0.679902</td>\n",
              "      <td>0.669265</td>\n",
              "      <td>0.671966</td>\n",
              "      <td>0.336899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.672106</td>\n",
              "      <td>0.674597</td>\n",
              "      <td>0.670546</td>\n",
              "      <td>0.672106</td>\n",
              "      <td>0.208023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.672106</td>\n",
              "      <td>0.678621</td>\n",
              "      <td>0.671827</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.142405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.676338</td>\n",
              "      <td>0.677340</td>\n",
              "      <td>0.672384</td>\n",
              "      <td>0.673498</td>\n",
              "      <td>0.146400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.673442</td>\n",
              "      <td>0.676004</td>\n",
              "      <td>0.672997</td>\n",
              "      <td>0.675892</td>\n",
              "      <td>0.116960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33025</th>\n",
              "      <td>0.921706</td>\n",
              "      <td>0.923321</td>\n",
              "      <td>0.916412</td>\n",
              "      <td>0.916921</td>\n",
              "      <td>0.291367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33026</th>\n",
              "      <td>0.916921</td>\n",
              "      <td>0.918476</td>\n",
              "      <td>0.910940</td>\n",
              "      <td>0.913751</td>\n",
              "      <td>0.294602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33027</th>\n",
              "      <td>0.913571</td>\n",
              "      <td>0.916024</td>\n",
              "      <td>0.909923</td>\n",
              "      <td>0.911119</td>\n",
              "      <td>0.261749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33028</th>\n",
              "      <td>0.911149</td>\n",
              "      <td>0.911717</td>\n",
              "      <td>0.902748</td>\n",
              "      <td>0.904480</td>\n",
              "      <td>0.302225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33029</th>\n",
              "      <td>0.904779</td>\n",
              "      <td>0.909085</td>\n",
              "      <td>0.901430</td>\n",
              "      <td>0.904719</td>\n",
              "      <td>0.684395</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33030 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5f2d786-241a-4fb8-91d0-d482c88f1088')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e5f2d786-241a-4fb8-91d0-d482c88f1088 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e5f2d786-241a-4fb8-91d0-d482c88f1088');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR4MT63c5T-1"
      },
      "outputs": [],
      "source": [
        "def addTI(df,lag=5):\n",
        "  short_w = [5,8,13,26,44,50,75,100,150]\n",
        "  long_w = [12,19,26,44,75,100,125,200,300]\n",
        "  data = df.copy()\n",
        "  features = []\n",
        "  for x,y in zip(short_w,long_w):\n",
        "    data[\"SMA_diff_\"+str(x)+\"_\"+str(y)] = data.Close.rolling(x).mean() - data.Close.rolling(y).mean()\n",
        "    features.append(\"SMA_diff_\"+str(x)+\"_\"+str(y))\n",
        "  for x in (short_w+long_w):\n",
        "    data[\"Boll_\"+str(x)] = (data.Close - data.Close.rolling(x).mean())/data.Close.rolling(x).std()\n",
        "    features.append(\"Boll_\"+str(x))\n",
        "    data[\"Min_\"+str(x)] = data.Close.rolling(x).min()/data.Close -1\n",
        "    features.append(\"Min_\"+str(x))\n",
        "    data[\"Max_\"+str(x)] =  data.Close.rolling(x).max()/data.Close -1\n",
        "    features.append(\"Max_\"+str(x))\n",
        "    data[\"Mom_\"+str(x)] = data.Close_pct_change.rolling(x).mean()\n",
        "    features.append(\"Mom_\"+str(x))\n",
        "    data[\"Vol_\"+str(x)] = data.Close_pct_change.rolling(x).std()\n",
        "    features.append(\"Vol_\"+str(x))\n",
        "\n",
        "  for f in features:\n",
        "    col = \"{}_lag_{}\".format(f,lag)\n",
        "    data[col] = data[f].shift(lag)\n",
        "\n",
        "  data.dropna(inplace=True)\n",
        "  return data\n",
        "def predictDF(df, model, name):\n",
        "  data = pd.DataFrame()\n",
        "  data[name] = model.predict(df)\n",
        "  return data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training and Voting**"
      ],
      "metadata": {
        "id": "x1g9VopQu88Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from pandas.core.window.expanding import Axis\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from pandas.core.algorithms import mode\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "accuracy_val = []\n",
        "positive_rt_val = []\n",
        "precision_val = []\n",
        "batchSize = 15 # number of table to concat\n",
        "\n",
        "print(\"number of rows = \"+str(len(stock_df)))\n",
        "stock_df = pd.concat([stock_df, table], ignore_index=True)\n",
        "\"\"\"========================================================================================TASK 1============================================================================================== \"\"\"\n",
        "stock_df['Close_pct_change'] = stock_df['Close'].pct_change().shift(-1) #NEW CHANGE\n",
        "stock_df = stock_df.dropna()\n",
        "bin_labels = [\"decrease\", \"no big change\", \"increase\"]\n",
        "stock_df['Close_pct_change_range'] = pd.qcut(stock_df['Close_pct_change'], q=3)\n",
        "stock_df['Close_pct_change_category'] = pd.qcut(stock_df['Close_pct_change'], q=3,labels=bin_labels)\n",
        "\"\"\"========================================================================================TASK 2============================================================================================== \"\"\"\n",
        "stock_df = addTI(stock_df,lag=21) #Task 2\n",
        "stock_df.dropna(inplace=True)\n",
        "stock_df.Close_pct_change_category = stock_df.Close_pct_change_category.apply(lambda x:1 if(x== \"increase\")else 0)\n",
        "stock_df.drop(columns=\"Close_pct_change_range\",inplace = True)  # droping the close_pct_change_Range colums as we dont need this need more to calcautet the big increase, decrease\n",
        "Y = stock_df.Close_pct_change_category.to_frame() # Y is labes for big change or small changes\n",
        "stock_df.drop(columns=\"Close_pct_change_category\",inplace = True)\n",
        "X= stock_df.copy() # X = are features  from all the indicator\n",
        "percentage_split =0.7;\n",
        "row= int(stock_df.shape[0]*percentage_split)\n",
        "\n",
        "#create the train data set\n",
        "X_train = X[:row]\n",
        "\n",
        "Y_train = Y[:row]\n",
        "\n",
        "#create the test data set\n",
        "X_test = X[row:]\n",
        "Y_test = Y[row:]\n",
        "\n",
        "# imported the libary for the machine learning model which is  support Vector Classifier (SVC)\n",
        "\n",
        "\n",
        "\n",
        "params = {'C': [500, 1000, 2000,1000], 'kernel': ['linear', 'rbf', 'sigmoid'], 'gamma': [500, 1000, 2000,1000]}\n",
        "\"\"\"========================================================================================TASK 3============================================================================================== \"\"\"\n",
        "\n",
        "# Now creating model\n",
        "model = SVC()\n",
        "grid_search = GridSearchCV(model, params, cv=5)\n",
        "\n",
        "model.fit(X_train,Y_train)  #training the model on X_train and Y_Train\n",
        "model.score(X_train,Y_train) # checking the score the model on train data set\n",
        "\n",
        "model.score(X_test,Y_test)# checking the score the model on test data set\n",
        "\n",
        "svc_pred = predictDF(X_test, model, \"Prediction_SVC\")\n",
        "\n",
        "#creating a second model\n",
        "model_RFR = RandomForestClassifier()\n",
        "model_RFR.fit(X_train,Y_train)\n",
        "\n",
        "model_RFR.score(X_train,Y_train) # Random\n",
        "\n",
        "model_RFR.score(X_test,Y_test)\n",
        "\n",
        "#Now we make and show the models predictins\n",
        "#stock_df['Prediction_RFR'] = model.predict(X)\n",
        "rfr_pred = predictDF(X_test, model_RFR, \"Prediction_RFR\")\n",
        "\n",
        "#creating a second model\n",
        "model_LR = LogisticRegression()\n",
        "model_LR.fit(X_train,Y_train)\n",
        "\n",
        "model_LR.score(X_train,Y_train) # logisticRegresssion  on train data\n",
        "\n",
        "model_LR.score(X_test,Y_test) # logistic Regression on test data\n",
        "\n",
        "lr_pred = predictDF(X_test, model_LR, \"Prediction_LR\")\n",
        "\n",
        "model_DTR = DecisionTreeClassifier()\n",
        "model_DTR.fit(X_train,Y_train)\n",
        "\n",
        "model_DTR.score(X_train,Y_train) # logisticRegresssion  on train data\n",
        "\n",
        "model_DTR.score(X_test,Y_test) # logisticRegresssion  on train data\n",
        "\n",
        "dtr_pred = predictDF(X_test, model_DTR, \"Prediction_DTR\")\n",
        "\n",
        "\n",
        "\n",
        "sum(dtr_pred['Prediction_DTR'])\n",
        "\n",
        "\n",
        "\n",
        "#creating the fifth model for prediction\n",
        "\n",
        "model_DNN = MLPClassifier()\n",
        "model_DNN= MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
        "\n",
        "model_DNN.fit(X_train,Y_train)\n",
        "\n",
        "model_DNN.score(X_train,Y_train)#DNN ON Train data\n",
        "\n",
        "model_DNN.score(X_test,Y_test) # DNN on the test data\n",
        "\n",
        "dnn_pred = predictDF(X_test, model_DNN, \"Prediction_DNN\")\n",
        "sum(dnn_pred['Prediction_DNN'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=7)\n",
        "knn.fit(X_train, Y_train)\n",
        "\n",
        "knn.score(X_train,Y_train)\n",
        "\n",
        "knn.score(X_test,Y_test)\n",
        "\n",
        "knn_pred = predictDF(X_test, knn, \"Prediction_KNN\")\n",
        "sum(knn_pred['Prediction_KNN'])\n",
        "\n",
        "\"\"\"### **Seventh Modelf ro predicition GaussianNB MODEL-7**\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Generate sample data for demonstration purposes\n",
        "# Create a Gaussian Naive Bayes classifier object\n",
        "classifier = GaussianNB()\n",
        "\n",
        "# Fit the classifier to the sample data\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "classifier.score(X_train,Y_train)\n",
        "\n",
        "classifier.score(X_test,Y_test)\n",
        "\n",
        "class_pred = predictDF(X_test, classifier, \"Prediction_Classifier\")\n",
        "sum(class_pred['Prediction_Classifier'])\n",
        "\n",
        "\"\"\"### **Eight Model for predicition XGBOOST MODEL-8**\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Generate sample data for demonstration purposes\n",
        "# Split the data into training and test sets\n",
        "\n",
        "\n",
        "# Create an XGBoost classifier object\n",
        "classifier_XGB = XGBClassifier()\n",
        "classifier_XGB.fit(X_train, Y_train)\n",
        "\n",
        "# Fit the classifier to the train\n",
        "\n",
        "classifier_XGB.score(X_train,Y_train)\n",
        "\n",
        "classifier_XGB.score(X_test,Y_test)\n",
        "\n",
        "XGB_pred = predictDF(X_test, classifier_XGB, \"Prediction_XGB\")\n",
        "sum(XGB_pred['Prediction_XGB'])\n",
        "\n",
        "\"\"\"### **Nineth Model fro predicition ADABoostClassifier -9**\"\"\"\n",
        "\n",
        "clf_ADA = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "clf_ADA.fit(X_train, Y_train)\n",
        "\n",
        "clf_ADA.score(X_train,Y_train)\n",
        "\n",
        "clf_ADA.score(X_test,Y_test)\n",
        "\n",
        "ADA_pred = predictDF(X_test, clf_ADA, \"Prediction_ADA\")\n",
        "sum(ADA_pred['Prediction_ADA'])\n",
        "\n",
        "\"\"\"### **Tenth Model for predicition  MODEL-10**\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Train the Gradient Boosting model\n",
        "model_GD = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0)\n",
        "model_GD.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "\n",
        "model_GD.score(X_train,Y_train)\n",
        "\n",
        "model_GD.score(X_test,Y_test)\n",
        "\n",
        "GD_pred = predictDF(X_test, model_GD, \"Prediction_GD\")\n",
        "sum(GD_pred['Prediction_GD'])\n",
        "\n",
        "Label1_For_GD = confusion_matrix(Y_test, model_GD.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _GD.predict\n",
        "TP_GD = Label1_For_GD[0,0]\n",
        "FP_GD = Label1_For_GD[0,1]\n",
        "TN_GD = Label1_For_GD[1,1]\n",
        "FN_GD = Label1_For_GD[1,0]\n",
        "precision_GD = TP_GD/(TP_GD+FP_GD)\n",
        "accuracy_GD = (TP_GD+TN_GD)/(TP_GD+FN_GD+FP_GD+TN_GD)\n",
        "positive_rt_GD = TP_GD/(TP_GD+FN_GD+FP_GD+TN_GD)\n",
        "\n",
        "print(\"Precision for GD_model: \",precision_GD)\n",
        "print(\"accuracy for GD_model: \",accuracy_GD)\n",
        "print(\"positive_rate for GD model: \", positive_rt_GD)\n",
        "\n",
        "Label1_For_ADA = confusion_matrix(Y_test, clf_ADA.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_ADA = Label1_For_ADA[0,0]\n",
        "FP_ADA = Label1_For_ADA[0,1]\n",
        "TN_ADA = Label1_For_ADA[1,1]\n",
        "FN_ADA = Label1_For_ADA[1,0]\n",
        "precision_ADA = TP_ADA/(TP_ADA+FP_ADA)\n",
        "accuracy_ADA = (TP_ADA+TN_ADA)/(TP_ADA+FN_ADA+FP_ADA+TN_ADA)\n",
        "positive_rt_ADA = TP_ADA/(TP_ADA+FN_ADA+FP_ADA+TN_ADA)\n",
        "\n",
        "print(\"Precision for ADD_model: \",precision_ADA)\n",
        "print(\"accuracy for ADD_model: \", accuracy_ADA)\n",
        "print(\"positive_rate for ADD model: \", positive_rt_ADA)\n",
        "\n",
        "\n",
        "Label1_For_XGB = confusion_matrix(Y_test, classifier_XGB.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_XGB = Label1_For_XGB[0,0]\n",
        "FP_XGB = Label1_For_XGB[0,1]\n",
        "TN_XGB = Label1_For_XGB[1,1]\n",
        "FN_XGB = Label1_For_XGB[1,0]\n",
        "precision_XGB = TP_XGB/(TP_XGB+FP_XGB)\n",
        "accuracy_XGB = (TP_XGB+TN_XGB)/(TP_XGB+FN_XGB+FP_XGB+TN_XGB)\n",
        "positive_rt_XGB = TP_XGB/(TP_XGB+FN_XGB+FP_XGB+TN_XGB)\n",
        "\n",
        "print(\"Precision for XGB_model: \",precision_XGB)\n",
        "print(\"accuracy for XGB_model: \", accuracy_XGB)\n",
        "print(\"positive_rate for XGB model: \", positive_rt_XGB)\n",
        "\n",
        "\n",
        "Label1_For_GNB = confusion_matrix(Y_test, classifier.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_GNB = Label1_For_GNB[0,0]\n",
        "FP_GNB = Label1_For_GNB[0,1]\n",
        "TN_GNB = Label1_For_GNB[1,1]\n",
        "FN_GNB = Label1_For_GNB[1,0]\n",
        "precision_GNB = TP_GNB/(TP_GNB+FP_GNB)\n",
        "accuracy_GNB = (TP_GNB+TN_GNB)/(TP_GNB+FN_GNB+FP_GNB+TN_GNB)\n",
        "positive_rt_GNB = TP_GNB/(TP_GNB+FN_GNB+FP_GNB+TN_GNB)\n",
        "\n",
        "print(\"Precision for GNB_model: \",precision_GNB)\n",
        "print(\"accuracy for GNB_model: \", accuracy_GNB)\n",
        "print(\"positive_rate for GNB model: \", positive_rt_GNB)\n",
        "\n",
        "\n",
        "Label1_For_KNN = confusion_matrix(Y_test, knn.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_KNN = Label1_For_KNN[0,0]\n",
        "FP_KNN = Label1_For_KNN[0,1]\n",
        "TN_KNN = Label1_For_KNN[1,1]\n",
        "FN_KNN = Label1_For_KNN[1,0]\n",
        "precision_KNN = TP_KNN/(TP_KNN+FP_KNN)\n",
        "accuracy_KNN = (TP_KNN+TN_KNN)/(TP_KNN+FN_KNN+FP_KNN+TN_KNN)\n",
        "positive_rt_KNN = TP_KNN/(TP_KNN+FN_KNN+FP_KNN+TN_KNN)\n",
        "\n",
        "print(\"Precision for KNN_model: \",precision_KNN)\n",
        "print(\"accuracy for KNN_model: \", accuracy_KNN)\n",
        "print(\"positive_rate for KNN model: \", positive_rt_KNN)\n",
        "\n",
        "Label1_For_DNN = confusion_matrix(Y_test, model_DNN.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_DNN = Label1_For_DNN[0,0]\n",
        "FP_DNN = Label1_For_DNN[0,1]\n",
        "TN_DNN = Label1_For_DNN[1,1]\n",
        "FN_DNN = Label1_For_DNN[1,0]\n",
        "\n",
        "precision_DNN = TP_DNN/(TP_DNN+FP_DNN)\n",
        "accuracy_DNN = (TP_DNN+TN_DNN)/(TP_DNN+FN_DNN+FP_DNN+TN_DNN)\n",
        "positive_rt_DNN = TP_DNN/(TP_DNN+FN_DNN+FP_DNN+TN_DNN)\n",
        "\n",
        "print(\"Precision for DNN_model: \",precision_DNN)\n",
        "print(\"accuracy for DNN_model: \", accuracy_DNN)\n",
        "print(\"positive_rate for DNN model: \", positive_rt_DNN)\n",
        "\n",
        "Label1_For_DTR = confusion_matrix(Y_test, model_DTR.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_DTR = Label1_For_DTR[0,0]\n",
        "FP_DTR = Label1_For_DTR[0,1]\n",
        "TN_DTR = Label1_For_DTR[1,1]\n",
        "FN_DTR = Label1_For_DTR[1,0]\n",
        "\n",
        "precision_DTR = TP_DTR/(TP_DTR+FP_DTR)\n",
        "accuracy_DTR = (TP_DTR+TN_DTR)/(TP_DTR+FN_DTR+FP_DTR+TN_DTR)\n",
        "positive_rt_DTR = TP_DTR/(TP_DTR+FN_DTR+FP_DTR+TN_DTR)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Precision for DTR_model: \",precision_DTR)\n",
        "print(\"accuracy for DTR_model : \", accuracy_DTR)\n",
        "print(\"postive rate for DTR: \", positive_rt_DTR)\n",
        "\n",
        "Label1_For_LR = confusion_matrix(Y_test, model_LR.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_LR = Label1_For_LR[0,0]\n",
        "FP_LR = Label1_For_LR[0,1]\n",
        "TN_LR = Label1_For_LR[1,1]\n",
        "FN_LR = Label1_For_LR[1,0]\n",
        "\n",
        "\n",
        "precision_LR = TP_LR/(TP_LR+FP_LR)\n",
        "\n",
        "\n",
        "accuracy_LR = (TP_LR+TN_LR)/(TP_LR+FN_LR+FP_LR+TN_LR)\n",
        "positive_rt_LR = TP_LR/(TP_LR+FN_LR+FP_LR+TN_LR)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Precision for LR_model: \",precision_LR)\n",
        "print(\"accuracy for LR_model : \", accuracy_LR)\n",
        "print(\"postive rate for LR: \", positive_rt_LR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Label1_For_RFR = confusion_matrix(Y_test, model_RFR.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_RFR = Label1_For_RFR[0,0]\n",
        "FP_RFR = Label1_For_RFR[0,1]\n",
        "TN_RFR = Label1_For_RFR[1,1]\n",
        "FN_RFR = Label1_For_RFR[1,0]\n",
        "\n",
        "precision_RFR = TP_RFR/(TP_RFR+FP_RFR)\n",
        "\n",
        "accuracy_RFR = (TP_RFR+TN_RFR)/(TP_RFR+FN_RFR+FP_RFR+TN_RFR)\n",
        "positive_rt_RFR = TP_RFR/(TP_RFR+FN_RFR+FP_RFR+TN_RFR)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Precision for RFR_model: \",precision_RFR)\n",
        "print(\"accuracy for RFR_model : \", accuracy_RFR)\n",
        "print(\"postive rate for RFR: \", positive_rt_RFR)\n",
        "\n",
        "\n",
        "Label1_For_SVC = confusion_matrix(Y_test, model.predict(X_test))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_SVC = Label1_For_SVC[0,0]\n",
        "FP_SVC = Label1_For_SVC[0,1]\n",
        "TN_SVC = Label1_For_SVC[1,1]\n",
        "FN_SVC = Label1_For_SVC[1,0]\n",
        "\n",
        "precision_SVC = TP_SVC/(TP_SVC+FP_SVC)\n",
        "\n",
        "accuracy_SVC = (TP_SVC+TN_SVC)/(TP_SVC+FN_SVC+FP_SVC+TN_SVC)\n",
        "positive_rt_SVC = TP_SVC/(TP_SVC+FN_SVC+FP_SVC+TN_SVC)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Precision for SVC_model: \",precision_SVC)\n",
        "print(\"accuracy for SVC_model : \", accuracy_SVC)\n",
        "print(\"postive rate for SVC: \", positive_rt_SVC)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"========================================================================================TASK 4: Feature Selection============================================================================================== \"\"\"\n",
        "\n",
        "\n",
        "pca = PCA(n_components=0.98) ## We used 0.95 to restore 95 % of intformation\n",
        "\n",
        "\n",
        "X_pca = pca.fit_transform(X)\n",
        "#shape ## This gave us that out of 95 features 3 features are that which will be important for us to selection  from X\n",
        "\n",
        "components = pca.components_\n",
        "\n",
        "components.shape\n",
        "\n",
        "PCA_summary = pd.DataFrame(columns=['model name', 'accuracy', 'precision','Postive rate'])\n",
        "\n",
        "\"\"\"Again Spliting the  data set  with THE PCA(Principal Component Analysis)\"\"\"\n",
        "\n",
        "# Get the percentage to split the data( 80% data for training set, and 10% data for test set)\n",
        "\n",
        "percentage_split =0.8;\n",
        "row= int(X_pca.shape[0]*percentage_split)\n",
        "\n",
        "#create the train data set\n",
        "X_train_pca = X_pca[:row]\n",
        "\n",
        "Y_train_pca = Y[:row]\n",
        "\n",
        "#create the test data set\n",
        "X_test_pca = X_pca[row:]\n",
        "Y_test_pca = Y[row:]\n",
        "\n",
        "\n",
        "\n",
        "model_GD = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0)  # MODEL fit !!!!!!! for model_GD\n",
        "model_GD.fit(X_train_pca, Y_train_pca)\n",
        "\n",
        "Label1_For_GD = confusion_matrix(Y_test_pca, model_GD.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _GD.predict\n",
        "TP_GD = Label1_For_GD[0,0]\n",
        "FP_GD = Label1_For_GD[0,1]\n",
        "TN_GD = Label1_For_GD[1,1]\n",
        "FN_GD = Label1_For_GD[1,0]\n",
        "precision_GD = TP_GD/(TP_GD+FP_GD)\n",
        "accuracy_GD = (TP_GD+TN_GD)/(TP_GD+FN_GD+FP_GD+TN_GD)\n",
        "positive_rt_GD = TP_GD/(TP_GD+FN_GD+FP_GD+TN_GD)\n",
        "\n",
        "\n",
        "#confusion_matrix(Y_test_pca, model_GD.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _GD.predict\n",
        "#GD = confusion_matrix(Y_test_pca, model_GD.predict(X_test_pca))\n",
        "\n",
        "print(\"\\n\\n------------GradientBoostingClassifier-------------\\n\\n\")\n",
        "print(\"score: \" +str(model_GD.score(X_test_pca,Y_test_pca)))   ## model _SCORE FRO GD\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, model_GD.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_GD))   ## model _SCORE FRO GD\n",
        "\n",
        "#PCA_summary.loc[0] = ['GraduentBoostingClassifier', str(model_GD.score(X_test_pca,Y_test_pca)), str(precision_score(Y_test_pca, model_GD.predict(X_test_pca)))]\n",
        "\n",
        "#print(GD)\n",
        "\n",
        "\"\"\"### **Model set for CLF_ADA(AdaaBoost) for  n = 9 (98% retain variance) with PCA_Spilt - for model 9**\"\"\"\n",
        "\n",
        "clf_ADA = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "clf_ADA.fit(X_train_pca, Y_train_pca)\n",
        "confusion_matrix(Y_test_pca, clf_ADA.predict(X_test_pca))\n",
        "ada = confusion_matrix(Y_test_pca, clf_ADA.predict(X_test_pca))\n",
        "clf_ADA.score(X_train_pca,Y_train_pca)\n",
        "\n",
        "\n",
        "\n",
        "Label1_For_ADA = confusion_matrix(Y_test_pca, clf_ADA.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_ADA = Label1_For_ADA[0,0]\n",
        "FP_ADA = Label1_For_ADA[0,1]\n",
        "TN_ADA = Label1_For_ADA[1,1]\n",
        "FN_ADA = Label1_For_ADA[1,0]\n",
        "precision_ADA = TP_ADA/(TP_ADA+FP_ADA)\n",
        "accuracy_ADA = (TP_ADA+TN_ADA)/(TP_ADA+FN_ADA+FP_ADA+TN_ADA)\n",
        "positive_rt_ADA = TP_ADA/(TP_ADA+FN_ADA+FP_ADA+TN_ADA)\n",
        "\n",
        "\n",
        "#print(\"positive_rate for ADD model: \\n\", positive_rt_ADA)\n",
        "print(\"\\n\\n------------AdaBoostClassifier-------------\\n\\n\")\n",
        "print(\"score: \" +str(clf_ADA.score(X_test_pca,Y_test_pca)))   ## model _SCORE FRO GD\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, clf_ADA.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_ADA))\n",
        "#PCA_summary.loc[1] = ['AdaBoostClassifier', str(clf_ADA.score(X_test_pca,Y_test_pca)), str(precision_score(Y_test_pca, clf_ADA.predict(X_test_pca)))]\n",
        "\n",
        "\n",
        "\n",
        "classifier_XGB = XGBClassifier()\n",
        "classifier_XGB.fit(X_train_pca, Y_train_pca)\n",
        "confusion_matrix(Y_test_pca, classifier_XGB.predict(X_test_pca))\n",
        "xgb =confusion_matrix(Y_test_pca, classifier_XGB.predict(X_test_pca))\n",
        "classifier_XGB.score(X_train_pca,Y_train_pca)\n",
        "\n",
        "\n",
        "Label1_For_XGB = confusion_matrix(Y_test_pca, classifier_XGB.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_XGB = Label1_For_XGB[0,0]\n",
        "FP_XGB = Label1_For_XGB[0,1]\n",
        "TN_XGB = Label1_For_XGB[1,1]\n",
        "FN_XGB = Label1_For_XGB[1,0]\n",
        "precision_XGB = TP_XGB/(TP_XGB+FP_XGB)\n",
        "accuracy_XGB = (TP_XGB+TN_XGB)/(TP_XGB+FN_XGB+FP_XGB+TN_XGB)\n",
        "positive_rt_XGB = TP_XGB/(TP_XGB+FN_XGB+FP_XGB+TN_XGB)\n",
        "\n",
        "\n",
        "#print(\"positive_rate for XGB model: \\n\", positive_rt_XGB)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------XGBClassifier-------------\\n\\n\")\n",
        "print(\"score: \" +str(classifier_XGB.score(X_test_pca,Y_test_pca)))   ## model _SCORE FRO GD\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, classifier_XGB.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_XGB))\n",
        "#PCA_summary.loc[2] = ['XGBClassifier', str(classifier_XGB.score(X_test_pca,Y_test_pca)),str(precision_score(Y_test_pca, classifier_XGB.predict(X_test_pca)))]\n",
        "\n",
        "\"\"\"### **Model set for GaussianNB for  n = 9 (98% retain variance) with PCA_Spilt- for model 7**\"\"\"\n",
        "\n",
        "classifier = GaussianNB()\n",
        "\n",
        "# Fit the classifier to the sample data\n",
        "classifier.fit(X_train_pca, Y_train_pca)\n",
        "confusion_matrix(Y_test_pca, classifier.predict(X_test_pca))\n",
        "gnb = confusion_matrix(Y_test_pca, classifier.predict(X_test_pca))\n",
        "classifier.score(X_train_pca,Y_train_pca)\n",
        "\n",
        "\n",
        "Label1_For_GNB = confusion_matrix(Y_test_pca, classifier.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_GNB = Label1_For_GNB[0,0]\n",
        "FP_GNB = Label1_For_GNB[0,1]\n",
        "TN_GNB = Label1_For_GNB[1,1]\n",
        "FN_GNB = Label1_For_GNB[1,0]\n",
        "precision_GNB = TP_GNB/(TP_GNB+FP_GNB)\n",
        "accuracy_GNB = (TP_GNB+TN_GNB)/(TP_GNB+FN_GNB+FP_GNB+TN_GNB)\n",
        "positive_rt_GNB = TP_GNB/(TP_GNB+FN_GNB+FP_GNB+TN_GNB)\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------GaussianNB-------------\\n\\n\")\n",
        "print(\"score: \" +str(classifier.score(X_test_pca,Y_test_pca)))   ## model _SCORE FRO GD\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, classifier.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_GNB))\n",
        "#PCA_summary.loc[3] = ['GaussianNB',str(classifier.score(X_test_pca,Y_test_pca)),str(precision_score(Y_test_pca, classifier.predict(X_test_pca)))]\n",
        "\n",
        "\"\"\"### **Model set for KNN for  n = 9 (98% retain variance) with PCA_Spilt- for model 6**\"\"\"\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=7)\n",
        "knn.fit(X_train_pca, Y_train_pca)\n",
        "confusion_matrix(Y_test_pca, knn.predict(X_test_pca))\n",
        "knn_Value = confusion_matrix(Y_test_pca, knn.predict(X_test_pca))\n",
        "knn.score(X_train_pca,Y_train_pca)\n",
        "\n",
        "Label1_For_KNN = confusion_matrix(Y_test_pca, knn.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_KNN = Label1_For_KNN[0,0]\n",
        "FP_KNN = Label1_For_KNN[0,1]\n",
        "TN_KNN = Label1_For_KNN[1,1]\n",
        "FN_KNN = Label1_For_KNN[1,0]\n",
        "precision_KNN = TP_KNN/(TP_KNN+FP_KNN)\n",
        "accuracy_KNN = (TP_KNN+TN_KNN)/(TP_KNN+FN_KNN+FP_KNN+TN_KNN)\n",
        "positive_rt_KNN = TP_KNN/(TP_KNN+FN_KNN+FP_KNN+TN_KNN)\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------KNeighborsClassifier-------------\\n\\n\")\n",
        "print(\"score: \" +str(knn.score(X_test_pca,Y_test_pca)))   ## model _SCORE FRO GD\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, knn.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_KNN))\n",
        "\n",
        "#PCA_summary.loc[4] = ['KNeighborsClassifier',str(knn.score(X_test_pca,Y_test_pca)),str(precision_score(Y_test_pca, knn.predict(X_test_pca)))]\n",
        "\n",
        "\"\"\"### **Model set for DNN for  n = 9 (98% retain variance) with PCA_Spilt- for model 5**\"\"\"\n",
        "\n",
        "model_DNN = MLPClassifier()\n",
        "model_DNN= MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
        "model_DNN.fit(X_train_pca,Y_train_pca)\n",
        "confusion_matrix(Y_test_pca, model_DNN.predict(X_test_pca))\n",
        "dnn = confusion_matrix(Y_test_pca, model_DNN.predict(X_test_pca))\n",
        "model_DNN.score(X_train_pca,Y_train_pca)\n",
        "\n",
        "Label1_For_DNN = confusion_matrix(Y_test_pca, model_DNN.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_DNN = Label1_For_DNN[0,0]\n",
        "FP_DNN = Label1_For_DNN[0,1]\n",
        "TN_DNN = Label1_For_DNN[1,1]\n",
        "FN_DNN = Label1_For_DNN[1,0]\n",
        "\n",
        "precision_DNN = TP_DNN/(TP_DNN+FP_DNN)\n",
        "accuracy_DNN = (TP_DNN+TN_DNN)/(TP_DNN+FN_DNN+FP_DNN+TN_DNN)\n",
        "positive_rt_DNN = TP_DNN/(TP_DNN+FN_DNN+FP_DNN+TN_DNN)\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------MLPClassifier-------------\\n\\n\")\n",
        "print(\"score: \" +str(model_DNN.score(X_test_pca,Y_test_pca)))   ## model _SCORE FRO GD\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, model_DNN.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_DNN))\n",
        "# PCA_summary.loc[5] = ['MLPClassifier',str(model_DNN.score(X_test_pca,Y_test_pca)),str(precision_score(Y_test_pca, model_DNN.predict(X_test_pca)))]\n",
        "\n",
        "\"\"\"### **Model set for DTR for  n = 9 (98% retain variance) with PCA_Spilt- for model 4**\"\"\"\n",
        "\n",
        "model_DTR = DecisionTreeClassifier()\n",
        "model_DTR.fit(X_train_pca,Y_train_pca)\n",
        "confusion_matrix(Y_test_pca, model_DTR.predict(X_test_pca))\n",
        "dtr = confusion_matrix(Y_test_pca, model_DTR.predict(X_test_pca))\n",
        "model_DTR.score(X_train_pca,Y_train_pca)\n",
        "\n",
        "\n",
        "Label1_For_DTR = confusion_matrix(Y_test_pca, model_DTR.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_DTR = Label1_For_DTR[0,0]\n",
        "FP_DTR = Label1_For_DTR[0,1]\n",
        "TN_DTR = Label1_For_DTR[1,1]\n",
        "FN_DTR = Label1_For_DTR[1,0]\n",
        "\n",
        "precision_DTR = TP_DTR/(TP_DTR+FP_DTR)\n",
        "accuracy_DTR = (TP_DTR+TN_DTR)/(TP_DTR+FN_DTR+FP_DTR+TN_DTR)\n",
        "positive_rt_DTR = TP_DTR/(TP_DTR+FN_DTR+FP_DTR+TN_DTR)\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------DecisionTreeClassifier-------------\\n\\n\")\n",
        "print(\"score: \" +str(model_DTR.score(X_test_pca,Y_test_pca)))   ## model _SCORE FRO GD\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, model_DTR.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_DTR))\n",
        "\n",
        "#PCA_summary.loc[6] = ['DecisionTreeClassifier',str(model_DTR.score(X_test_pca,Y_test_pca)),str(precision_score(Y_test_pca, model_DTR.predict(X_test_pca)))]\n",
        "\n",
        "\n",
        "\n",
        "model_LR = LogisticRegression()\n",
        "model_LR.fit(X_train_pca,Y_train_pca)\n",
        "confusion_matrix(Y_test_pca, model_LR.predict(X_test_pca))\n",
        "\n",
        "lr = confusion_matrix(Y_test_pca, model_LR.predict(X_test_pca))\n",
        "model_LR.score(X_train_pca,Y_train_pca)\n",
        "\n",
        "Label1_For_LR = confusion_matrix(Y_test_pca, model_LR.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_LR = Label1_For_LR[0,0]\n",
        "FP_LR = Label1_For_LR[0,1]\n",
        "TN_LR = Label1_For_LR[1,1]\n",
        "FN_LR = Label1_For_LR[1,0]\n",
        "\n",
        "\n",
        "precision_LR = TP_LR/(TP_LR+FP_LR)\n",
        "\n",
        "\n",
        "accuracy_LR = (TP_LR+TN_LR)/(TP_LR+FN_LR+FP_LR+TN_LR)\n",
        "positive_rt_LR = TP_LR/(TP_LR+FN_LR+FP_LR+TN_LR)\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------LogisticRegression-------------\\n\\n\")\n",
        "print(\"score: \" +str(model_LR.score(X_test_pca,Y_test_pca)))   ## model _SCORE FRO GD\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, model_LR.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_LR))\n",
        "#PCA_summary.loc[7] = ['LogisticRegression',str(model_LR.score(X_test_pca,Y_test_pca)),str(precision_score(Y_test_pca, model_LR.predict(X_test_pca)))]\n",
        "\n",
        "\n",
        "\n",
        "model_RFR = RandomForestClassifier()\n",
        "model_RFR.fit(X_train_pca,Y_train_pca)\n",
        "confusion_matrix(Y_test_pca, model_RFR.predict(X_test_pca))\n",
        "\n",
        "rfr =confusion_matrix(Y_test_pca, model_RFR.predict(X_test_pca))\n",
        "model_RFR.score(X_train_pca,Y_train_pca)\n",
        "\n",
        "\n",
        "Label1_For_RFR = confusion_matrix(Y_test_pca, model_RFR.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_RFR = Label1_For_RFR[0,0]\n",
        "FP_RFR = Label1_For_RFR[0,1]\n",
        "TN_RFR = Label1_For_RFR[1,1]\n",
        "FN_RFR = Label1_For_RFR[1,0]\n",
        "\n",
        "precision_RFR = TP_RFR/(TP_RFR+FP_RFR)\n",
        "\n",
        "accuracy_RFR = (TP_RFR+TN_RFR)/(TP_RFR+FN_RFR+FP_RFR+TN_RFR)\n",
        "positive_rt_RFR = TP_RFR/(TP_RFR+FN_RFR+FP_RFR+TN_RFR)\n",
        "\n",
        "print(\"\\n\\n------------RandomForestClassifier-------------\\n\\n\")\n",
        "print(\"score: \" +str(model_RFR.score(X_test_pca,Y_test_pca)))   ## model _SCORE FRO GD\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, model_RFR.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_RFR))\n",
        "\n",
        "#PCA_summary.loc[8] = ['RandomForestClassifier',str(model_RFR.score(X_test_pca,Y_test_pca)),str(precision_score(Y_test_pca, model_RFR.predict(X_test_pca)))]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train_pca,Y_train_pca)\n",
        "confusion_matrix(Y_test_pca, model.predict(X_test_pca))\n",
        "svc_val = confusion_matrix(Y_test_pca, model.predict(X_test_pca))\n",
        "model.score(X_train_pca,Y_train_pca)\n",
        "\n",
        "\n",
        "Label1_For_SVC = confusion_matrix(Y_test_pca, model.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_SVC = Label1_For_SVC[0,0]\n",
        "FP_SVC = Label1_For_SVC[0,1]\n",
        "TN_SVC = Label1_For_SVC[1,1]\n",
        "FN_SVC = Label1_For_SVC[1,0]\n",
        "\n",
        "precision_SVC = TP_SVC/(TP_SVC+FP_SVC)\n",
        "\n",
        "accuracy_SVC = (TP_SVC+TN_SVC)/(TP_SVC+FN_SVC+FP_SVC+TN_SVC)\n",
        "positive_rt_SVC = TP_SVC/(TP_SVC+FN_SVC+FP_SVC+TN_SVC)\n",
        "\n",
        "\n",
        "print(\"\\n\\n------------SVC-------------\\n\\n\")\n",
        "print(\"score: \" +str(model.score(X_test_pca,Y_test_pca)))\n",
        "print(\"precision: \"+str(precision_score(Y_test_pca, model.predict(X_test_pca))))\n",
        "print(\"postive rate : \"+ str(positive_rt_SVC))\n",
        "\n",
        "#PCA_summary.loc[9] = ['SVC',str(model.score(X_test_pca,Y_test_pca)),str(precision_score(Y_test_pca, model.predict(X_test_pca)))]\n",
        "\n",
        "\n",
        "\n",
        "#PCA_summary\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"========================================================================================Task 5: Model Ensembling============================================================================================== \"\"\"\n",
        "\n",
        "estimators = [('Gradient',model_GD),\n",
        "            ('SVC',model),\n",
        "            ('RFR',model_RFR),\n",
        "            ('DNN',model_DNN),\n",
        "            ('ADABoost',clf_ADA),\n",
        "            ('GaussianNB',classifier),\n",
        "            ('GradXGBoostient',classifier_XGB),\n",
        "            ('Logistic Regression', model_LR)\n",
        "            ]\n",
        "\n",
        "\n",
        "voteHard = VotingClassifier(estimators, voting=\"hard\")\n",
        "voteHard.fit(X_train_pca,Y_train_pca)\n",
        "\n",
        "voteHard_pred = voteHard.predict(X_test_pca)\n",
        "\n",
        "Label1_For_voteHard = confusion_matrix(Y_test_pca, voteHard.predict(X_test_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_V = Label1_For_voteHard[0,0]\n",
        "FP_V = Label1_For_voteHard[0,1]\n",
        "TN_V = Label1_For_voteHard[1,1]\n",
        "FN_V = Label1_For_voteHard[1,0]\n",
        "\n",
        "precision_V = TP_V/(TP_V+FP_V)\n",
        "\n",
        "accuracy_V = (TP_V+TN_V)/(TP_V+FN_V+FP_V+TN_V)\n",
        "positive_rt_V = TP_V/(TP_V+FN_V+FP_V+TN_V)\n",
        "print(\"\\n\\n------------VOTE HARD-------------\\n\\n\")\n",
        "print(\"score_ensemble \" +str(accuracy_V))   ## model _SCORE FRO GD\n",
        "print(\"precision_ensemble: \"+str(precision_V))\n",
        "print(\"Postive_rate_ensemble: \" +str(positive_rt_V))\n",
        "\n",
        "#voteHard 15mins\n",
        "# score_ensemble 0.6794614723574907\n",
        "# precision_ensemble: 0.9997892074198989\n",
        "# Postive_rate_ensemble: 0.6793182469206531\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJJpbvezug03",
        "outputId": "03584a06-8e3c-4f98-8550-675d1e5f5b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of rows = 33030\n",
            "Precision for GD_model:  1.0\n",
            "accuracy for GD_model:  0.9998090327508832\n",
            "positive_rate for GD model:  0.6948343359113912\n",
            "Precision for ADD_model:  1.0\n",
            "accuracy for ADD_model:  0.9998090327508832\n",
            "positive_rate for ADD model:  0.6948343359113912\n",
            "Precision for XGB_model:  1.0\n",
            "accuracy for XGB_model:  0.9998090327508832\n",
            "positive_rate for XGB model:  0.6948343359113912\n",
            "Precision for GNB_model:  0.9796619486051944\n",
            "accuracy for GNB_model:  0.6863362933256947\n",
            "positive_rate for GNB model:  0.6807027594767497\n",
            "Precision for KNN_model:  0.8202555998350969\n",
            "accuracy for KNN_model:  0.6257041917311181\n",
            "positive_rate for KNN model:  0.5699417549890193\n",
            "Precision for DNN_model:  0.9418716504053868\n",
            "accuracy for DNN_model:  0.8178172443425953\n",
            "positive_rate for DNN model:  0.654444762723193\n",
            "Precision for DTR_model:  1.0\n",
            "accuracy for DTR_model :  0.9998090327508832\n",
            "postive rate for DTR:  0.6948343359113912\n",
            "Precision for LR_model:  0.9968393568778343\n",
            "accuracy for LR_model :  0.7118304210827843\n",
            "postive rate for LR:  0.6926382125465482\n",
            "Precision for RFR_model:  1.0\n",
            "accuracy for RFR_model :  0.9996180655017665\n",
            "postive rate for RFR:  0.6948343359113912\n",
            "Precision for SVC_model:  0.9995877422014566\n",
            "accuracy for SVC_model :  0.6946433686622744\n",
            "postive rate for SVC:  0.694547885037716\n",
            "\n",
            "\n",
            "------------GradientBoostingClassifier-------------\n",
            "\n",
            "\n",
            "score: 0.6794614723574907\n",
            "precision: 0.0\n",
            "postive rate : 0.6794614723574907\n",
            "\n",
            "\n",
            "------------AdaBoostClassifier-------------\n",
            "\n",
            "\n",
            "score: 0.6723002005156116\n",
            "precision: 0.33974358974358976\n",
            "postive rate : 0.6647092523632198\n",
            "\n",
            "\n",
            "------------XGBClassifier-------------\n",
            "\n",
            "\n",
            "score: 0.6539673446004011\n",
            "precision: 0.33935018050541516\n",
            "postive rate : 0.6270409624749356\n",
            "\n",
            "\n",
            "------------GaussianNB-------------\n",
            "\n",
            "\n",
            "score: 0.6681466628473217\n",
            "precision: 0.2348993288590604\n",
            "postive rate : 0.6631337725580063\n",
            "\n",
            "\n",
            "------------KNeighborsClassifier-------------\n",
            "\n",
            "\n",
            "score: 0.611142938985964\n",
            "precision: 0.3099601593625498\n",
            "postive rate : 0.5554282440561443\n",
            "\n",
            "\n",
            "------------MLPClassifier-------------\n",
            "\n",
            "\n",
            "score: 0.6794614723574907\n",
            "precision: 0.0\n",
            "postive rate : 0.6794614723574907\n",
            "\n",
            "\n",
            "------------DecisionTreeClassifier-------------\n",
            "\n",
            "\n",
            "score: 0.5567172729876826\n",
            "precision: 0.3210855949895616\n",
            "postive rate : 0.4465769120595818\n",
            "\n",
            "\n",
            "------------LogisticRegression-------------\n",
            "\n",
            "\n",
            "score: 0.6794614723574907\n",
            "precision: 0.0\n",
            "postive rate : 0.6794614723574907\n",
            "\n",
            "\n",
            "------------RandomForestClassifier-------------\n",
            "\n",
            "\n",
            "score: 0.6682898882841593\n",
            "precision: 0.3511450381679389\n",
            "postive rate : 0.6551131480951017\n",
            "\n",
            "\n",
            "------------SVC-------------\n",
            "\n",
            "\n",
            "score: 0.6791750214838155\n",
            "precision: 0.25\n",
            "postive rate : 0.6790317960469779\n",
            "\n",
            "\n",
            "------------VOTE HARD-------------\n",
            "\n",
            "\n",
            "score_ensemble 0.6793182469206531\n",
            "precision_ensemble: 0.9995784148397976\n",
            "Postive_rate_ensemble: 0.6791750214838155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6"
      ],
      "metadata": {
        "id": "BcegdNmwKVeS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHCzPWlWQGKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a825b7c-3ad4-4944-b00f-4c08431e744e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of table =  2447\n",
            "number of rows =  2175383\n",
            "score_ensemble_test1 0.6656029193884955\n",
            "precision_ensemble_test1: 0.9938681281995748\n",
            "Postive_rate_ensemble_test1: 0.662579056808703\n"
          ]
        }
      ],
      "source": [
        "object_test = open(r'/content/testing_set1.pkl', \"rb\")  #Test file 1\n",
        "df_test = pickle.load(object_test)\n",
        "\n",
        "\n",
        "stock_df_test = pd.DataFrame()\n",
        "limit = len(df_test) # number of table to concat\n",
        "print(\"number of table = \",limit)\n",
        "\n",
        "# Loop through the list of tables and concatenate them vertically into stock_df_test\n",
        "for i in range (limit):\n",
        "  table = df_test[i]\n",
        "  stock_df_test = pd.concat([stock_df_test, table], ignore_index=True)\n",
        "\n",
        "print(\"number of rows = \",len(stock_df_test))\n",
        "stock_df_test['Close_pct_change'] = stock_df_test['Close'].pct_change().shift(-1) #NEW CHANGE\n",
        "stock_df_test = addTI(stock_df_test,lag=21) # adds features\n",
        "stock_df_test.dropna(inplace=True)\n",
        "\n",
        "stock_df_test['Close_pct_change'] = stock_df_test['Close'].pct_change().shift(-1) #NEW CHANGE\n",
        "\n",
        "bin_labels = [\"decrease\", \"no big change\", \"increase\"]\n",
        "stock_df_test['Close_pct_change_range'] = pd.qcut(stock_df_test['Close_pct_change'], q=3)\n",
        "stock_df_test['Close_pct_change_category'] = pd.qcut(stock_df_test['Close_pct_change'], q=3,labels=bin_labels)\n",
        "stock_df_test.dropna(inplace=True)\n",
        "\n",
        "stock_df_test.Close_pct_change_category = stock_df_test.Close_pct_change_category.apply(lambda x:1 if(x== \"increase\")else 0)\n",
        "stock_df_test.drop(columns=\"Close_pct_change_range\",inplace = True)  # droping the  close_pct_change_Range colums as we dont need this need more to calcautet the big increase, decrease\n",
        "Y = stock_df_test.Close_pct_change_category.to_frame() # Y is labes for big change or small changes\n",
        "stock_df_test.drop(columns=\"Close_pct_change_category\",inplace = True)\n",
        "X = stock_df_test.copy() # X = are features  from all the indicator (does not contain labels)\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#For testing, we only transform pca based on training dataset\n",
        "X_pca = pca.transform(X)\n",
        "\n",
        "Label1_For_voteHard = confusion_matrix(Y, voteHard.predict(X_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_V = Label1_For_voteHard[0,0]\n",
        "FP_V = Label1_For_voteHard[0,1]\n",
        "TN_V = Label1_For_voteHard[1,1]\n",
        "FN_V = Label1_For_voteHard[1,0]\n",
        "\n",
        "precision_V = TP_V/(TP_V+FP_V)\n",
        "\n",
        "accuracy_V = (TP_V+TN_V)/(TP_V+FN_V+FP_V+TN_V)\n",
        "positive_rt_V = TP_V/(TP_V+FN_V+FP_V+TN_V)\n",
        "\n",
        "print(\"score_ensemble_test1 \" +str(accuracy_V))   ## model _SCORE FRO GD\n",
        "print(\"precision_ensemble_test1: \"+str(precision_V))\n",
        "print(\"Postive_rate_ensemble_test1: \" +str(positive_rt_V))\n",
        "\n",
        "\n",
        "# output: approx 50 mins\n",
        "# number of table =  2447\n",
        "# number of rows =  2175383\n",
        "# score_ensemble_test1 0.6656029193884955\n",
        "# precision_ensemble_test1: 0.9938681281995748\n",
        "# Postive_rate_ensemble_test1: 0.662579056808703"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w67leMec1ux"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y16peJsYn1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5749707-bca7-44c3-c53e-b0803d737d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of table =  705\n",
            "number of rows =  1552410\n",
            "score_ensemble_test2 0.6658914339035128\n",
            "precision_ensemble_test2: 0.9943017492035015\n",
            "Postive_rate_ensemble_test2: 0.6628678328023343\n"
          ]
        }
      ],
      "source": [
        "object_test = open(r'/content/testing_set2.pkl', \"rb\")  #Test file 2\n",
        "df_test = pickle.load(object_test)\n",
        "\n",
        "stock_df_test = pd.DataFrame()\n",
        "limit = len(df_test) # number of table to concat\n",
        "print(\"number of table = \",limit)\n",
        "\n",
        "# Loop through the list of tables and concatenate them vertically into stock_df_test\n",
        "for i in range (limit):\n",
        "  table = df_test[i]\n",
        "  stock_df_test = pd.concat([stock_df_test, table], ignore_index=True)\n",
        "\n",
        "print(\"number of rows = \",len(stock_df_test))\n",
        "stock_df_test['Close_pct_change'] = stock_df_test['Close'].pct_change().shift(-1) #NEW CHANGE\n",
        "stock_df_test = addTI(stock_df_test,lag=21) # adds features\n",
        "stock_df_test.dropna(inplace=True)\n",
        "\n",
        "stock_df_test['Close_pct_change'] = stock_df_test['Close'].pct_change().shift(-1) #NEW CHANGE\n",
        "\n",
        "bin_labels = [\"decrease\", \"no big change\", \"increase\"]\n",
        "stock_df_test['Close_pct_change_range'] = pd.qcut(stock_df_test['Close_pct_change'], q=3)\n",
        "stock_df_test['Close_pct_change_category'] = pd.qcut(stock_df_test['Close_pct_change'], q=3,labels=bin_labels)\n",
        "stock_df_test.dropna(inplace=True)\n",
        "\n",
        "stock_df_test.Close_pct_change_category = stock_df_test.Close_pct_change_category.apply(lambda x:1 if(x== \"increase\")else 0)\n",
        "stock_df_test.drop(columns=\"Close_pct_change_range\",inplace = True)  # droping the  close_pct_change_Range colums as we dont need this need more to calcautet the big increase, decrease\n",
        "Y = stock_df_test.Close_pct_change_category.to_frame() # Y is labes for big change or small changes\n",
        "stock_df_test.drop(columns=\"Close_pct_change_category\",inplace = True)\n",
        "X = stock_df_test.copy() # X = are features  from all the indicator (does not contain labels)\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#For testing, we only transform pca based on training dataset\n",
        "X_pca = pca.transform(X)\n",
        "\n",
        "Label1_For_voteHard = confusion_matrix(Y, voteHard.predict(X_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_V = Label1_For_voteHard[0,0]\n",
        "FP_V = Label1_For_voteHard[0,1]\n",
        "TN_V = Label1_For_voteHard[1,1]\n",
        "FN_V = Label1_For_voteHard[1,0]\n",
        "\n",
        "precision_V = TP_V/(TP_V+FP_V)\n",
        "\n",
        "accuracy_V = (TP_V+TN_V)/(TP_V+FN_V+FP_V+TN_V)\n",
        "positive_rt_V = TP_V/(TP_V+FN_V+FP_V+TN_V)\n",
        "\n",
        "print(\"score_ensemble_test2 \" +str(accuracy_V))   ## model _SCORE FRO GD\n",
        "print(\"precision_ensemble_test2: \"+str(precision_V))\n",
        "print(\"Postive_rate_ensemble_test2: \" +str(positive_rt_V))\n",
        "\n",
        "#output : approx 45-50 mins\n",
        "# number of table =  705\n",
        "# number of rows =  1552410\n",
        "# score_ensemble_test2 0.6658914339035128\n",
        "# precision_ensemble_test2: 0.9943017492035015\n",
        "# Postive_rate_ensemble_test2: 0.6628678328023343"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhzKk24XYvI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a294b13a-60a3-4ecb-b145-abbbfe0a1d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of table =  4598\n",
            "number of rows =  2220834\n",
            "score_ensemble_test3 0.6625958613829986\n",
            "precision_ensemble_test3: 0.9879716146051533\n",
            "Postive_rate_ensemble_test3: 0.6586483364806619\n"
          ]
        }
      ],
      "source": [
        "object_test = open(r'/content/testing_set3.pkl', \"rb\") #Test file 3\n",
        "df_test = pickle.load(object_test)\n",
        "\n",
        "stock_df_test = pd.DataFrame()\n",
        "limit = len(df_test) # number of table to concat\n",
        "print(\"number of table = \",limit)\n",
        "\n",
        "# Loop through the list of tables and concatenate them vertically into stock_df_test\n",
        "for i in range (limit):\n",
        "  table = df_test[i]\n",
        "  stock_df_test = pd.concat([stock_df_test, table], ignore_index=True)\n",
        "\n",
        "print(\"number of rows = \",len(stock_df_test))\n",
        "stock_df_test['Close_pct_change'] = stock_df_test['Close'].pct_change()\n",
        "stock_df_test = addTI(stock_df_test,lag=21)\n",
        "stock_df_test.dropna(inplace=True)\n",
        "\n",
        "stock_df_test['Close_pct_change'] = stock_df_test['Close'].pct_change()\n",
        "\n",
        "bin_labels = [\"decrease\", \"no big change\", \"increase\"]\n",
        "stock_df_test['Close_pct_change_range'] = pd.qcut(stock_df_test['Close_pct_change'], q=3)\n",
        "stock_df_test['Close_pct_change_category'] = pd.qcut(stock_df_test['Close_pct_change'], q=3,labels=bin_labels)\n",
        "stock_df_test.dropna(inplace=True)\n",
        "\n",
        "stock_df_test.Close_pct_change_category = stock_df_test.Close_pct_change_category.apply(lambda x:1 if(x== \"increase\")else 0)\n",
        "stock_df_test.drop(columns=\"Close_pct_change_range\",inplace = True)  # droping the  close_pct_change_Range colums as we dont need this need more to calcautet the big increase, decrease\n",
        "Y = stock_df_test.Close_pct_change_category.to_frame() # Y is labes for big change or small changes\n",
        "stock_df_test.drop(columns=\"Close_pct_change_category\",inplace = True)\n",
        "X = stock_df_test.copy() # X = are features  from all the indicator\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#For testing, we only transform pca based on training dataset\n",
        "X_pca = pca.transform(X)\n",
        "\n",
        "Label1_For_voteHard = confusion_matrix(Y, voteHard.predict(X_pca))   ## Confusion matrix  for classfication precision  for Model _LR\n",
        "\n",
        "TP_V = Label1_For_voteHard[0,0]\n",
        "FP_V = Label1_For_voteHard[0,1]\n",
        "TN_V = Label1_For_voteHard[1,1]\n",
        "FN_V = Label1_For_voteHard[1,0]\n",
        "\n",
        "precision_V = TP_V/(TP_V+FP_V)\n",
        "\n",
        "accuracy_V = (TP_V+TN_V)/(TP_V+FN_V+FP_V+TN_V)\n",
        "positive_rt_V = TP_V/(TP_V+FN_V+FP_V+TN_V)\n",
        "\n",
        "print(\"score_ensemble_test3 \" +str(accuracy_V))   ## model _SCORE FRO GD\n",
        "print(\"precision_ensemble_test3: \"+str(precision_V))\n",
        "print(\"Postive_rate_ensemble_test3: \" +str(positive_rt_V))\n",
        "#output runtime : approx 58 mins\n",
        "# number of table =  4598\n",
        "# number of rows =  2220834\n",
        "# score_ensemble_test3 0.6625958613829986\n",
        "# precision_ensemble_test3: 0.9879716146051533\n",
        "# Postive_rate_ensemble_test3: 0.6586483364806619"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}